{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-cGLb_s7WIX",
        "outputId": "2c9e751d-6106-4279-b6b1-ce1b91816bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 100, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: the plaintiff filed a complaint against the defendant for unauthorized land acquisition . the defendant is accused of unauthorized property acquisition... a land acquisition of a property .\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "\n",
        "# Example document summarization\n",
        "case_details = \"\"\"\n",
        "The plaintiff filed a complaint against the defendant for unauthorized land acquisition...\n",
        "\"\"\"\n",
        "summary = summarizer(case_details, max_length=100, min_length=30, do_sample=False)\n",
        "print(\"Summary:\", summary[0][\"summary_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_legal_references(query):\n",
        "    url = \"https://indiankanoon.org/search/\"\n",
        "    params = {\"formInput\": query}\n",
        "    response = requests.get(url, params=params)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    cases = soup.find_all(\"div\", class_=\"result_title\")\n",
        "    results = []\n",
        "    for case in cases:\n",
        "        title = case.find(\"a\").text\n",
        "        link = \"https://indiankanoon.org\" + case.find(\"a\")[\"href\"]\n",
        "        results.append({\"title\": title, \"link\": link})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example query for land dispute\n",
        "legal_references = fetch_legal_references(\"land dispute\")\n",
        "print(legal_references)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF-A2QSq7wAw",
        "outputId": "62086fe8-efab-4c9d-b783-c2cf2433ee29"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_legal_references(query, num_pages=1):\n",
        "    url = \"https://indiankanoon.org/search/\"\n",
        "    results = []\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        params = {\"formInput\": query, \"pagenum\": page}\n",
        "        response = requests.get(url, params=params)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        cases = soup.find_all(\"div\", class_=\"result_title\")\n",
        "        for case in cases:\n",
        "            title = case.find(\"a\").text\n",
        "            link = \"https://indiankanoon.org\" + case.find(\"a\")[\"href\"]\n",
        "            results.append({\"title\": title, \"link\": link})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example query for land dispute, fetching results from multiple pages\n",
        "legal_references = fetch_legal_references(\"land dispute\", num_pages=3)  # Adjust num_pages based on results\n",
        "print(legal_references)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S19rBt48aB4",
        "outputId": "c842f09c-e19b-4ae3-aba8-81825a83f8df"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viTm1YyI8wk4",
        "outputId": "fa952801-ba4b-4724-93fc-69e763f9a1ba"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.20)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N_K45mUO9A4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyppeteer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_UQCS4n-hJS",
        "outputId": "f053f72d-f285-4364-f0fb-a6b3568232f3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyppeteer in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (2024.12.14)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (8.5.0)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer) (10.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from pyppeteer import launch, errors\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "async def fetch_legal_references(query, num_pages=1):\n",
        "    # Launch a headless browser with timeout configuration\n",
        "    browser = await launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
        "    page = await browser.newPage()\n",
        "\n",
        "    url = \"https://indiankanoon.org/search/\"\n",
        "    results = []\n",
        "\n",
        "    try:\n",
        "        for page_num in range(1, num_pages + 1):\n",
        "            params = {\"formInput\": query, \"pagenum\": page_num}\n",
        "            full_url = f\"{url}?{'&'.join(f'{key}={val}' for key, val in params.items())}\"\n",
        "            await page.goto(full_url, timeout=30000)  # Increased timeout to 30 seconds\n",
        "\n",
        "            content = await page.content()\n",
        "            soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "            cases = soup.find_all(\"div\", class_=\"result_title\")\n",
        "            for case in cases:\n",
        "                title = case.find(\"a\").text\n",
        "                link = \"https://indiankanoon.org\" + case.find(\"a\")[\"href\"]\n",
        "                results.append({\"title\": title, \"link\": link})\n",
        "\n",
        "    except errors.BrowserError as e:\n",
        "        print(\"Browser closed unexpectedly:\", e)\n",
        "    finally:\n",
        "        await browser.close()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example query for land dispute\n",
        "async def main():\n",
        "    legal_references = await fetch_legal_references(\"land dispute\", num_pages=3)\n",
        "    print(legal_references)\n",
        "\n",
        "# Running it using %async magic in Jupyter or aw\n"
      ],
      "metadata": {
        "id": "wbg67GUK_JnB"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from pyppeteer import launch, errors\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "async def fetch_legal_references(query, num_pages=1):\n",
        "    results = []\n",
        "    attempts = 0\n",
        "    max_attempts = 3\n",
        "\n",
        "    while attempts < max_attempts:\n",
        "        try:\n",
        "            # Launch a headless browser with timeout configuration\n",
        "            browser = await launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
        "            page = await browser.newPage()\n",
        "\n",
        "            url = \"https://indiankanoon.org/search/\"\n",
        "\n",
        "            for page_num in range(1, num_pages + 1):\n",
        "                params = {\"formInput\": query, \"pagenum\": page_num}\n",
        "                full_url = f\"{url}?{'&'.join(f'{key}={val}' for key, val in params.items())}\"\n",
        "                await page.goto(full_url, timeout=30000)  # 30-second timeout\n",
        "\n",
        "                content = await page.content()\n",
        "                soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "                cases = soup.find_all(\"div\", class_=\"result_title\")\n",
        "                for case in cases:\n",
        "                    title = case.find(\"a\").text\n",
        "                    link = \"https://indiankanoon.org\" + case.find(\"a\")[\"href\"]\n",
        "                    results.append({\"title\": title, \"link\": link})\n",
        "\n",
        "            await browser.close()\n",
        "            break  # If successful, break out of the loop\n",
        "        except errors.BrowserError as e:\n",
        "            print(f\"Attempt {attempts + 1} failed: {e}\")\n",
        "            attempts += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error on attempt {attempts + 1}: {e}\")\n",
        "            attempts += 1\n",
        "\n",
        "    if attempts == max_attempts and not results:\n",
        "        print(\"Failed to fetch legal references after multiple attempts.\")\n",
        "    return results\n",
        "\n",
        "# Example query for land dispute\n",
        "async def main():\n",
        "    legal_references = await fetch_legal_references(\"land dispute\", num_pages=3)\n",
        "    print(legal_references)\n",
        "\n",
        "# Running it using %async magic in Jupyter or await in Colab\n",
        "# In Jupyter Notebook:\n",
        "# %async main()\n",
        "\n",
        "# In Google Colab:\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BV57c9A_aQh",
        "outputId": "77516623-5a32-4ff4-e113-805332f158fa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from pyppeteer import launch, errors\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "async def fetch_legal_references(query, num_pages=1):\n",
        "    results = []\n",
        "    attempts = 0\n",
        "    max_attempts = 5\n",
        "    base_url = \"https://indiankanoon.org/search/\"\n",
        "\n",
        "    while attempts < max_attempts:\n",
        "        try:\n",
        "            # Launch a headless browser with improved args for stability\n",
        "            browser = await launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
        "            page = await browser.newPage()\n",
        "\n",
        "            for page_num in range(1, num_pages + 1):\n",
        "                params = {\"formInput\": query, \"pagenum\": page_num}\n",
        "                full_url = f\"{base_url}?{'&'.join(f'{key}={val}' for key, val in params.items())}\"\n",
        "                print(f\"Accessing URL: {full_url}\")\n",
        "\n",
        "                await page.goto(full_url, timeout=45000)  # Increased timeout to 45 seconds\n",
        "                content = await page.content()\n",
        "                soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "                cases = soup.find_all(\"div\", class_=\"result_title\")\n",
        "                for case in cases:\n",
        "                    title = case.find(\"a\").text\n",
        "                    link = \"https://indiankanoon.org\" + case.find(\"a\")[\"href\"]\n",
        "                    results.append({\"title\": title, \"link\": link})\n",
        "\n",
        "            await browser.close()\n",
        "            break  # Exit loop if successful\n",
        "        except errors.BrowserError as e:\n",
        "            print(f\"Attempt {attempts + 1} failed: {e}\")\n",
        "            attempts += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error on attempt {attempts + 1}: {e}\")\n",
        "            attempts += 1\n",
        "\n",
        "    if attempts == max_attempts and not results:\n",
        "        print(\"Failed to fetch legal references after multiple attempts.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example query for land dispute\n",
        "async def main():\n",
        "    legal_references = await fetch_legal_references(\"land dispute\", num_pages=3)\n",
        "    print(legal_references)\n",
        "\n",
        "# Running it using %async magic in Jupyter or await in Colab\n",
        "# In Jupyter Notebook:\n",
        "# %async main()\n",
        "\n",
        "# In Google Colab:\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oGVqAlk_h1A",
        "outputId": "50ee7f25-14b6-4f20-b8ee-d193f217bc7a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accessing URL: https://indiankanoon.org/search/?formInput=land dispute&pagenum=1\n",
            "Accessing URL: https://indiankanoon.org/search/?formInput=land dispute&pagenum=2\n",
            "Accessing URL: https://indiankanoon.org/search/?formInput=land dispute&pagenum=3\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "async def fetch_legal_references(session, query, num_pages=1):\n",
        "    results = []\n",
        "    base_url = \"https://indiankanoon.org/search/\"\n",
        "\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}?formInput={query}&pagenum={page_num}\"\n",
        "        print(f\"Accessing URL: {url}\")\n",
        "\n",
        "        async with session.get(url) as response:\n",
        "            if response.status == 200:\n",
        "                page_content = await response.text()\n",
        "                soup = BeautifulSoup(page_content, \"html.parser\")\n",
        "                cases = soup.find_all(\"div\", class_=\"result_title\")\n",
        "\n",
        "                for case in cases:\n",
        "                    title = case.find(\"a\").text\n",
        "                    link = \"https://indiankanoon.org\" + case.find(\"a\")[\"href\"]\n",
        "                    results.append({\"title\": title, \"link\": link})\n",
        "            else:\n",
        "                print(f\"Failed to retrieve page {page_num}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "async def main():\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        legal_references = await fetch_legal_references(session, \"land dispute\", num_pages=3)\n",
        "        print(legal_references)\n",
        "\n",
        "# Run in Jupyter Notebook or Google Colab\n",
        "# In Jupyter Notebook:\n",
        "# %run script_name.py\n",
        "\n",
        "# In Google Colab:\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JT8l67sABuB",
        "outputId": "41ccdf75-34f1-4648-f08d-8705d5975a6b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accessing URL: https://indiankanoon.org/search/?formInput=land dispute&pagenum=1\n",
            "Failed to retrieve page 1\n",
            "Accessing URL: https://indiankanoon.org/search/?formInput=land dispute&pagenum=2\n",
            "Failed to retrieve page 2\n",
            "Accessing URL: https://indiankanoon.org/search/?formInput=land dispute&pagenum=3\n",
            "Failed to retrieve page 3\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"final_judge_database.csv\")\n"
      ],
      "metadata": {
        "id": "fnR3qPalDtL9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(\"/mnt/data\", exist_ok=True)\n",
        "\n",
        "# Move the file to the directory\n",
        "# Assuming the file is in the current working directory\n",
        "import shutil\n",
        "shutil.move(\"final_judge_database.csv\", \"/mnt/data/final_judge_database.csv\")\n",
        "\n",
        "# Now read the file\n",
        "df = pd.read_csv(\"/mnt/data/final_judge_database.csv\")\n"
      ],
      "metadata": {
        "id": "Irh1aE23D0e4"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current Directory:\", os.getcwd())\n",
        "print(\"Files in Current Directory:\", os.listdir(\".\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB9ZMk4wD4q6",
        "outputId": "9da38e05-113b-4ae7-d11a-572093d444b2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content\n",
            "Files in Current Directory: ['.config', 'prayer_clauses_dataset.csv', 'ipc_sections.csv', 'wandb', 'drive', 'prayer_clause_model', 'logs', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Display column names\n",
        "print(df.columns)\n",
        "\n",
        "# Check for null or missing values\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9pMGNm8ECl7",
        "outputId": "e780d09d-a098-4e61-fb6c-56b900723e76"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                         case title  \\\n",
            "0           0        Jagdish Saran & Ors vs Union Of India & Ors   \n",
            "1           1  Grindlays Bank Limited vs The Income Tax Offic...   \n",
            "2           2  Deep Chand And Anr. vs State Of Uttar Pradesh ...   \n",
            "3           3  Managing Director, Uttar Pradesh ... vs Vinay ...   \n",
            "4           4                   State Of Rajasthan vs Daulat Ram   \n",
            "\n",
            "           judges name(s)  date of judgment  \\\n",
            "0       Krishnaiyer, V.R.  28 January, 1980   \n",
            "1            Pathak, R.S.  15 January, 1980   \n",
            "2   N Untwalia, O C Reddy  16 January, 1980   \n",
            "3  Sarkaria, Ranjit Singh  16 January, 1980   \n",
            "4       A Koshal, S M Ali  23 January, 1980   \n",
            "\n",
            "                                            citation  \\\n",
            "0                   1980 AIR  820, 1980 SCR  (2) 831   \n",
            "1                   1980 AIR  656, 1980 SCR  (2) 765   \n",
            "2   AIR 1980 SC 633, (1980) 3 SCC 231, 1980 (12) ...   \n",
            "3                   1980 AIR  840, 1980 SCR  (2) 773   \n",
            "4   AIR 1980 SC 1314, 1980 CRILJ 929, (1980) 3 SC...   \n",
            "\n",
            "                                              issues  decision  \\\n",
            "0  Article 14 in The Constitution Of India 1949 ;...         0   \n",
            "1  Article 226 in The Constitution Of India 1949 ...         0   \n",
            "2     Section 4 in The Land Acquisition Act, 1894 ;          1   \n",
            "3  Article 226 in The Constitution Of India 1949 ...         0   \n",
            "4                                                NaN         0   \n",
            "\n",
            "                                         cited cases  \n",
            "0  {'state of kerala v. n. m. thomas ': 1.0, 'raj...  \n",
            "1  {'c- g] director of inspection of income tax (...  \n",
            "2                                                 {}  \n",
            "3  {'e-g] sukhdev singh v. bhagat ram ': 0.8, 'ra...  \n",
            "4                                                 {}  \n",
            "Index(['Unnamed: 0', 'case title', 'judges name(s)', 'date of judgment',\n",
            "       'citation', 'issues', 'decision', 'cited cases'],\n",
            "      dtype='object')\n",
            "Unnamed: 0             0\n",
            "case title             0\n",
            "judges name(s)         0\n",
            "date of judgment       0\n",
            "citation              27\n",
            "issues              1429\n",
            "decision               0\n",
            "cited cases            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVyTJM4oKzl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
        "\n",
        "# Example case text\n",
        "case_text = \"The petitioner seeks relief regarding a land dispute where the lower court ruled in favor of the respondent despite contradictory evidence.\"\n",
        "\n",
        "# Summarize the case\n",
        "summary = summarizer(case_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "print(\"Case Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPapZrAFEEVJ",
        "outputId": "48426ee6-e914-4a19-fcfc-195db5525964"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 100, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case Summary: the petitioner seeks relief regarding a land dispute . the lower court ruled in favor of the respondent despite contradictory evidence .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_citations(issue, dataset):\n",
        "    # Ensure column name is standardized\n",
        "    if 'Issues' not in dataset.columns:\n",
        "        raise ValueError(\"The 'Issues' column is missing. Check your dataset.\")\n",
        "\n",
        "    # Retrieve rows matching the issue\n",
        "    relevant_cases = dataset[dataset['Issues'].str.contains(issue, case=False, na=False)]\n",
        "    return relevant_cases[['Case Title', 'Cited Cases']]\n",
        "\n",
        "# Example usage\n",
        "issue = \"land dispute\"\n",
        "try:\n",
        "    citations = retrieve_citations(issue, df)\n",
        "    print(citations)\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAzc88OZEaMa",
        "outputId": "281dd34c-f49a-4e1b-a154-534ac04bb998"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'Issues' column is missing. Check your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_citations_alternative(keyword, dataset):\n",
        "    \"\"\"\n",
        "    Retrieve relevant cases by searching in the 'Case Title' or 'Cited Cases' columns.\n",
        "\n",
        "    Args:\n",
        "        keyword (str): Keyword to search for in the dataset.\n",
        "        dataset (pd.DataFrame): The case dataset.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Relevant rows from the dataset.\n",
        "    \"\"\"\n",
        "    # Check if columns exist\n",
        "    if 'Case Title' not in dataset.columns or 'Cited Cases' not in dataset.columns:\n",
        "        raise ValueError(\"Required columns ('Case Title', 'Cited Cases') are missing in the dataset.\")\n",
        "\n",
        "    # Filter rows based on keyword in 'Case Title' or 'Cited Cases'\n",
        "    relevant_cases = dataset[\n",
        "        dataset['Case Title'].str.contains(keyword, case=False, na=False) |\n",
        "        dataset['Cited Cases'].str.contains(keyword, case=False, na=False)\n",
        "    ]\n",
        "    return relevant_cases[['Case Title', 'Cited Cases']]\n",
        "\n",
        "# Example usage\n",
        "keyword = \"land dispute\"\n",
        "try:\n",
        "    citations = retrieve_citations_alternative(keyword, df)\n",
        "    print(\"Relevant Citations:\\n\", citations)\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM6Uy7RKEpca",
        "outputId": "44ec970f-3af4-47d0-efa6-044f18b38d3c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required columns ('Case Title', 'Cited Cases') are missing in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install exa-py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5ljEjd4FdcZ",
        "outputId": "4e48bd9d-d5df-4fbe-e08b-b426fa1eb2c2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: exa-py in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from exa-py) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from exa-py) (4.12.2)\n",
            "Collecting openai>=1.10.0 (from exa-py)\n",
            "  Using cached openai-1.59.5-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.10.0->exa-py) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->exa-py) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->exa-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->exa-py) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->exa-py) (2024.12.14)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.10.0->exa-py) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.10.0->exa-py) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.10.0->exa-py) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.10.0->exa-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.10.0->exa-py) (2.27.2)\n",
            "Using cached openai-1.59.5-py3-none-any.whl (454 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed openai-1.59.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import openai\n",
        "import json  # Import json module to fix the NameError\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"/mnt/data/final_judge_database.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Set up OpenAI API key\n",
        "openai.api_key = \"sk-your-openai-key\"\n",
        "\n",
        "# Step 3: Define the function to extract acts and sections\n",
        "def extract_relevant_sections(case_text):\n",
        "    relevant_sections = []\n",
        "    pattern = r\"(Section\\s\\d+|Act\\s\\d+|Clause\\s\\d+)(.*?)(\\n\\n|\\Z)\"\n",
        "    matches = re.findall(pattern, case_text, re.DOTALL)\n",
        "\n",
        "    for match in matches:\n",
        "        relevant_sections.append(match[0] + match[1])\n",
        "\n",
        "    return relevant_sections\n",
        "\n",
        "# Step 4: Define a function to query OpenAI for mapping and summaries\n",
        "def query_llm(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=300,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def map_acts_sections(relevant_sections):\n",
        "    mapped_data = []\n",
        "\n",
        "    for section in relevant_sections:\n",
        "        prompt = f\"Explain the following legal provision and its relevance:\\n\\n{section}\"\n",
        "        summary = query_llm(prompt)\n",
        "\n",
        "        mapped_data.append({\n",
        "            \"act_section\": section,\n",
        "            \"llm_summary\": summary\n",
        "        })\n",
        "\n",
        "    return mapped_data\n",
        "\n",
        "# Step 5: Process the dataset\n",
        "all_mapped_acts_sections = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    case_text = f\"Case Title: {row.get('Case Title', '')}\\n\" \\\n",
        "                f\"Judges: {row.get('Judges Name(s)', '')}\\n\" \\\n",
        "                f\"Decision: {row.get('Decision', '')}\\n\" \\\n",
        "                f\"Cited Cases: {row.get('Cited Cases', '')}\"\n",
        "\n",
        "    # Extract relevant sections from the case text\n",
        "    relevant_sections = extract_relevant_sections(case_text)\n",
        "\n",
        "    # Map the acts/sections to the extracted text using OpenAI\n",
        "    mapped_acts_sections = map_acts_sections(relevant_sections)\n",
        "    all_mapped_acts_sections.extend(mapped_acts_sections)\n",
        "\n",
        "# Step 6: Display the results\n",
        "print(json.dumps(all_mapped_acts_sections, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBUqrBEFxnh",
        "outputId": "68271503-69a1-4863-8a9a-79f5a6cd49f2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import openai\n",
        "\n",
        "def retry_query_llm(prompt, retries=3, backoff=10):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=300,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except openai.error.RateLimitError:\n",
        "            print(f\"Rate limit hit. Retrying in {backoff} seconds... (Attempt {attempt + 1})\")\n",
        "            time.sleep(backoff)\n",
        "    raise Exception(\"Exceeded maximum retries\")"
      ],
      "metadata": {
        "id": "eM2KDo_fKZ8r"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Map issues to IPC sections\n",
        "issue_to_ipc = {\n",
        "    \"land dispute\": [\"Section 420 - Cheating\", \"Section 34 - Common Intent\"],\n",
        "    \"contract dispute\": [\"Section 73 - Breach of Contract\"]\n",
        "}\n",
        "\n",
        "ipc_sections = issue_to_ipc.get(issue, [])\n",
        "print(\"Applicable IPC Sections:\", ipc_sections)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieWty8eNHlQp",
        "outputId": "14a5d2e9-5fad-4b54-d68e-c3bf97d4ee62"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applicable IPC Sections: ['Section 420 - Cheating', 'Section 34 - Common Intent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JQGYNDTLDeD",
        "outputId": "1668f87f-8484-49b1-f3e3-e2670bede0fa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unnamed: 0', 'case title', 'judges name(s)', 'date of judgment', 'citation', 'issues', 'decision', 'cited cases']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip().str.lower()\n"
      ],
      "metadata": {
        "id": "nH1_AFmnLGLC"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'issues' not in df.columns:\n",
        "    print(\"The 'Issues' column is missing. Check your dataset.\")\n",
        "else:\n",
        "    print(\"The 'Issues' column is present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3WrNgOgLIrr",
        "outputId": "2f01b8ab-e3cc-4ea2-b171-9018a3f764bd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'Issues' column is present.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJusdm9gLXWD"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_citations(issue, dataset):\n",
        "    # Ensure column name is standardized\n",
        "    if 'Issues' not in dataset.columns:\n",
        "        raise ValueError(\"The 'Issues' column is missing. Check your dataset.\")\n",
        "\n",
        "    # Retrieve rows matching the issue\n",
        "    relevant_cases = dataset[dataset['Issues'].str.contains(issue, case=False, na=False)]\n",
        "    return relevant_cases[['Case Title', 'Cited Cases']]\n",
        "\n",
        "# Example usage\n",
        "issue = \"land dispute\"\n",
        "try:\n",
        "    citations = retrieve_citations(issue, df)\n",
        "    print(citations)\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1b833e-dea2-4f6c-d5a0-a41ef1ebb8e8",
        "id": "vIYn2GgaLXjp"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'Issues' column is missing. Check your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Print the available columns\n",
        "print(\"Available columns in the dataset:\", df.columns.tolist())\n",
        "\n",
        "# Step 2: Check for leading/trailing spaces or inconsistencies\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "print(\"Standardized columns:\", df.columns.tolist())\n",
        "\n",
        "# Step 3: Check if the 'Issues' column is present\n",
        "if 'issues' not in df.columns:\n",
        "    print(\"The 'Issues' column is missing. Available columns after standardization:\", df.columns.tolist())\n",
        "else:\n",
        "    print(\"The 'Issues' column is present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwRw5NbCLnkE",
        "outputId": "a3c6e4f6-65ad-4ad3-be7b-b24dfd1e4200"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in the dataset: ['unnamed: 0', 'case title', 'judges name(s)', 'date of judgment', 'citation', 'issues', 'decision', 'cited cases']\n",
            "Standardized columns: ['unnamed: 0', 'case title', 'judges name(s)', 'date of judgment', 'citation', 'issues', 'decision', 'cited cases']\n",
            "The 'Issues' column is present.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_citations(issue, dataset):\n",
        "    # Standardize column names\n",
        "    dataset.columns = dataset.columns.str.strip().str.lower()  # Removes spaces and converts to lowercase\n",
        "    print(\"Standardized columns:\", dataset.columns.tolist())\n",
        "\n",
        "    # Ensure the 'issues' column is present after standardization\n",
        "    if 'issues' not in dataset.columns:\n",
        "        raise ValueError(\"The 'issues' column is missing. Check your dataset.\")\n",
        "\n",
        "    # Retrieve rows matching the issue\n",
        "    relevant_cases = dataset[dataset['issues'].str.contains(issue, case=False, na=False)]\n",
        "    return relevant_cases[['case title', 'cited cases']]\n",
        "\n",
        "# Example usage\n",
        "issue = \"land dispute\"\n",
        "try:\n",
        "    citations = retrieve_citations(issue, df)\n",
        "    print(\"Retrieved Citations:\")\n",
        "    print(citations)\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6D0SQgyMCDz",
        "outputId": "5ca30c06-cf3f-4ddd-c446-c69d6ab94586"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized columns: ['unnamed: 0', 'case title', 'judges name(s)', 'date of judgment', 'citation', 'issues', 'decision', 'cited cases']\n",
            "Retrieved Citations:\n",
            "Empty DataFrame\n",
            "Columns: [case title, cited cases]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_citations(issue, dataset):\n",
        "    # Standardize column names\n",
        "    dataset.columns = dataset.columns.str.strip().str.lower()  # Removes spaces and converts to lowercase\n",
        "    print(\"Standardized columns:\", dataset.columns.tolist())\n",
        "\n",
        "    # Ensure the 'issues' column is present after standardization\n",
        "    if 'issues' not in dataset.columns:\n",
        "        raise ValueError(\"The 'issues' column is missing. Check your dataset.\")\n",
        "\n",
        "    # Display a sample of the issues column for debugging\n",
        "    print(\"Sample of 'issues' column:\")\n",
        "    print(dataset['issues'].head())\n",
        "\n",
        "    # Retrieve rows matching the issue\n",
        "    relevant_cases = dataset[dataset['issues'].str.contains(issue, case=False, na=False)]\n",
        "\n",
        "    # Check if any cases are found\n",
        "    if relevant_cases.empty:\n",
        "        print(f\"No matches found for issue: '{issue}'. Please verify the input.\")\n",
        "    else:\n",
        "        print(f\"Found {len(relevant_cases)} matching cases for issue: '{issue}'.\")\n",
        "\n",
        "    return relevant_cases[['case title', 'cited cases']]\n",
        "\n",
        "# Example usage\n",
        "issue = \"murder\"\n",
        "try:\n",
        "    citations = retrieve_citations(issue, df)\n",
        "    print(\"Retrieved Citations:\")\n",
        "    print(citations)\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-rKJxliMMeq",
        "outputId": "2e463537-d502-41d3-899a-dcc29134155d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized columns: ['unnamed: 0', 'case title', 'judges name(s)', 'date of judgment', 'citation', 'issues', 'decision', 'cited cases']\n",
            "Sample of 'issues' column:\n",
            "0    Article 14 in The Constitution Of India 1949 ;...\n",
            "1    Article 226 in The Constitution Of India 1949 ...\n",
            "2       Section 4 in The Land Acquisition Act, 1894 ; \n",
            "3    Article 226 in The Constitution Of India 1949 ...\n",
            "4                                                  NaN\n",
            "Name: issues, dtype: object\n",
            "No matches found for issue: 'murder'. Please verify the input.\n",
            "Retrieved Citations:\n",
            "Empty DataFrame\n",
            "Columns: [case title, cited cases]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF8d43nrM-1r",
        "outputId": "4b2b201b-ce54-4ed5-b56c-130b00adbcab"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def setup_faiss_index(dataset, column_name):\n",
        "    \"\"\"\n",
        "    Create a FAISS index for the specified column in the dataset.\n",
        "    \"\"\"\n",
        "    # Standardize column names\n",
        "    dataset.columns = dataset.columns.str.strip().str.lower()\n",
        "    print(\"Standardized columns:\", dataset.columns.tolist())\n",
        "\n",
        "    if column_name not in dataset.columns:\n",
        "        raise ValueError(f\"The '{column_name}' column is missing. Check your dataset.\")\n",
        "\n",
        "    # Extract text from the column\n",
        "    text_data = dataset[column_name].dropna().tolist()\n",
        "\n",
        "    # Use TF-IDF to vectorize the text\n",
        "    vectorizer = TfidfVectorizer(max_features=512)  # Limit the number of features for efficiency\n",
        "    vectors = vectorizer.fit_transform(text_data).toarray()\n",
        "\n",
        "    # Create a FAISS index\n",
        "    dimension = vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(vectors).astype('float32'))\n",
        "\n",
        "    return index, vectorizer, text_data\n",
        "\n",
        "def faiss_search(index, vectorizer, query, text_data, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform a FAISS search to find the most similar items to the query.\n",
        "    \"\"\"\n",
        "    query_vector = vectorizer.transform([query]).toarray().astype('float32')\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "    results = [(text_data[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
        "    return results\n",
        "\n",
        "# Step 1: Set up FAISS index for the 'issues' column\n",
        "try:\n",
        "    index, vectorizer, text_data = setup_faiss_index(df, 'issues')\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "    exit()\n",
        "\n",
        "# Step 2: Perform a FAISS search for the query\n",
        "query = \"murder\"\n",
        "results = faiss_search(index, vectorizer, query, text_data, top_k=5)\n",
        "print(\"\\nFAISS Search Results:\")\n",
        "for result, distance in results:\n",
        "    print(f\"Issue: {result}, Distance: {distance}\")\n",
        "\n",
        "# Step 3: Filter dataset based on FAISS results and retrieve citations\n",
        "def retrieve_citations_faiss(results, dataset):\n",
        "    # Extract matching issues from FAISS results\n",
        "    matching_issues = [result for result, _ in results]\n",
        "\n",
        "    # Filter the dataset for rows containing matching issues\n",
        "    relevant_cases = dataset[dataset['issues'].isin(matching_issues)]\n",
        "\n",
        "    if relevant_cases.empty:\n",
        "        print(f\"No matches found for issues: {matching_issues}. Please verify the input.\")\n",
        "    else:\n",
        "        print(f\"Found {len(relevant_cases)} matching cases for issues: {matching_issues}.\")\n",
        "\n",
        "    return relevant_cases[['case title', 'cited cases']]\n",
        "\n",
        "try:\n",
        "    citations = retrieve_citations_faiss(results, df)\n",
        "    print(\"\\nRetrieved Citations:\")\n",
        "    print(citations)\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIDn9y68M6k8",
        "outputId": "9c8e9f4f-25b8-460c-a328-bde352f8608d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized columns: ['unnamed: 0', 'case title', 'judges name(s)', 'date of judgment', 'citation', 'issues', 'decision', 'cited cases']\n",
            "\n",
            "FAISS Search Results:\n",
            "Issue: article (whether before or after the commencement of section 55 of the Constitution ; , Distance: 0.9999998807907104\n",
            "Issue: rule 56 a fundamental rules ; , Distance: 0.9999998807907104\n",
            "Issue: Section 15 in The Registration Act, 1908 ; Section 16 in The Registration Act, 1908 ; Section 21 in The General Clauses Act, 1897 ; , Distance: 0.9999998807907104\n",
            "Issue: Section 4 in The Payment of Gratuity Act, 1972 ; , Distance: 0.9999998807907104\n",
            "Issue: Section 6 in The General Clauses Act, 1897 ; Section 14 in The General Clauses Act, 1897 ; , Distance: 0.9999998807907104\n",
            "Found 8 matching cases for issues: ['article (whether before or after the commencement of section 55 of the Constitution ; ', 'rule 56 a fundamental rules ; ', 'Section 15 in The Registration Act, 1908 ; Section 16 in The Registration Act, 1908 ; Section 21 in The General Clauses Act, 1897 ; ', 'Section 4 in The Payment of Gratuity Act, 1972 ; ', 'Section 6 in The General Clauses Act, 1897 ; Section 14 in The General Clauses Act, 1897 ; '].\n",
            "\n",
            "Retrieved Citations:\n",
            "                                             case title  \\\n",
            "197   Waman Rao & Ors. Etc. Etc vs Union Of India An...   \n",
            "415   Brij Bihari Lal Agarwal vs High Court Of Madhy...   \n",
            "460             Swadeshi Cotton Mills vs Union Of India   \n",
            "517   Lalappa Lingappa & Ors vs Laxmi Vishnu Textile...   \n",
            "526   Rameshchandra Kachardas Porwal & ... vs State ...   \n",
            "1496        J. D. Shrivastava vs State Of M.P. & Others   \n",
            "4499               M.P. Pradhan vs Union Of India & Ors   \n",
            "4858                  Prithipal Singh vs Union Of India   \n",
            "\n",
            "                                            cited cases  \n",
            "197   {'vithalrao udhaorao uttarwar v. state of maha...  \n",
            "415   {'c] union of india v. col ': 0.8, 'state of u...  \n",
            "460   {'keshav mills co. ltd. v. union of india ': 0...  \n",
            "517   {'delhi cloth and general mills co. v. its wor...  \n",
            "526   {'tulsipur sugar co. v. notified area committe...  \n",
            "1496  {'d.ramaswami v. state of tamil nadu ': 0.8, '...  \n",
            "4499                                                 {}  \n",
            "4858  {'des raj and ors. v. state of punjab and ors ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Set up OpenAI API key\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Step 2: Function to extract IPC sections using regex\n",
        "def extract_ipc_sections(text):\n",
        "    \"\"\"\n",
        "    Extract IPC sections from the provided text using regex.\n",
        "    \"\"\"\n",
        "    pattern = r\"Section\\s\\d+\"\n",
        "    matches = re.findall(pattern, text)\n",
        "    return matches\n",
        "\n",
        "# Step 3: Query OpenAI API to generate IPC descriptions\n",
        "def get_ipc_details(section):\n",
        "    \"\"\"\n",
        "    Use OpenAI API to generate IPC section details.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"Provide the legal description and context for {section} of the Indian Penal Code (IPC).\"\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"text-davinci-003\",  # Change engine based on your requirements\n",
        "            prompt=prompt,\n",
        "            max_tokens=150,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        description = response.choices[0].text.strip()\n",
        "        return description\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for {section}: {e}\")\n",
        "        return \"Description unavailable.\"\n",
        "\n",
        "# Step 4: Map IPC sections to descriptions\n",
        "def map_ipc_sections_to_case(case_text):\n",
        "    \"\"\"\n",
        "    Extract IPC sections from the case text and map them to their descriptions.\n",
        "    \"\"\"\n",
        "    ipc_sections = extract_ipc_sections(case_text)\n",
        "    ipc_mapping = {}\n",
        "    for section in ipc_sections:\n",
        "        if section not in ipc_mapping:  # Avoid duplicate API calls\n",
        "            ipc_mapping[section] = get_ipc_details(section)\n",
        "    return ipc_mapping\n",
        "\n",
        "# Step 5: Apply the process to a dataset\n",
        "def process_case_file(dataset, text_column='issues'):\n",
        "    \"\"\"\n",
        "    Process the dataset to extract and map IPC sections for each case.\n",
        "    \"\"\"\n",
        "    dataset['ipc_mappings'] = dataset[text_column].apply(map_ipc_sections_to_case)\n",
        "    return dataset\n",
        "\n",
        "# Step 6: Example case dataset\n",
        "data = {\n",
        "    \"case_title\": [\"Case A\", \"Case B\"],\n",
        "    \"issues\": [\n",
        "        \"This case involves Section 302 and Section 307 for murder and attempted murder.\",\n",
        "        \"Fraudulent activities under Section 420 are highlighted in this case.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Process the dataset\n",
        "df = process_case_file(df, text_column='issues')\n",
        "\n",
        "# Step 7: Display results\n",
        "print(\"\\nProcessed Dataset with IPC Mappings:\")\n",
        "print(df[['case_title', 'ipc_mappings']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoNyk6uKOmg7",
        "outputId": "310e2fb2-7af1-4f2f-e8d6-a9744b62ddf6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching details for Section 302: The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations\n",
            "Error fetching details for Section 307: The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations\n",
            "Error fetching details for Section 420: The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations\n",
            "\n",
            "Processed Dataset with IPC Mappings:\n",
            "  case_title                                       ipc_mappings\n",
            "0     Case A  {'Section 302': 'Description unavailable.', 'S...\n",
            "1     Case B        {'Section 420': 'Description unavailable.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neZutfLjQirM",
        "outputId": "0da4d0f3-f317-4190-f2a8-ec3cdb2ac72f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.59.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def draft_petition(case_title, summary, citations, ipc_sections, prayer, interim_relief=None):\n",
        "    petition = f\"\"\"\n",
        "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
        "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
        "\n",
        "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
        "\n",
        "    TITLE: {case_title}\n",
        "\n",
        "    **Facts of the Case:**\n",
        "    {summary}\n",
        "\n",
        "    **Relevant Citations:**\n",
        "    {citations.to_string(index=False)}\n",
        "\n",
        "    **Applicable Legal Provisions:**\n",
        "    {\", \".join(ipc_sections)}\n",
        "\n",
        "    **Prayer Clause:**\n",
        "    {prayer}\n",
        "\n",
        "    \"\"\"\n",
        "    if interim_relief:\n",
        "        petition += f\"\\n**Interim Relief Sought:**\\n{interim_relief}\"\n",
        "\n",
        "    return petition\n",
        "\n",
        "# Example Usage\n",
        "case_title = \"Sample Case Title\"\n",
        "prayer = \"The petitioner prays for appropriate relief, including compensation.\"\n",
        "interim_relief = \"Stay order on eviction.\"\n",
        "\n",
        "petition = draft_petition(case_title, summary, citations, ipc_sections, prayer, interim_relief)\n",
        "print(petition)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OZcZGxPQztZ",
        "outputId": "7de07050-2567-4dcb-8746-e9211c4e9860"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Sample Case Title\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    the petitioner seeks relief regarding a land dispute . the lower court ruled in favor of the respondent despite contradictory evidence .\n",
            "\n",
            "    **Relevant Citations:**\n",
            "                                                                 case title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    cited cases\n",
            "                    Waman Rao & Ors. Etc. Etc vs Union Of India And Ors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'vithalrao udhaorao uttarwar v. state of maharashtra ': 0.8, 'dattatraya govind mahajan v. state of maharashtra ': 0.8, 'smt. indira nehru gandhi v. raj narain ': 0.8, 'shankari prasad v. union of india ': 0.8, 'sajjansingh v. state of rajasthan ': 0.8, 'i.c. golakanath v. union of india & ors ': 1.0, 'h. h. kesavananda bharati sripadagalavaru v. state of kerala ': 1.0, 'thumati venkaiah v. state of a.p ': 1.0, 'tuticorin v. t. s. d. nadar ': 1.0, 'vithalrao udhaorao uttarwar v. state of maharashtra the high ': 1.0, 'smt. indira gandhi v. raj narain art ': 1.0, 'in shankari prasad v. union of india ': 1.0, 'in sajjansingh v. state of rajasthan ': 1.0, 'in i.t.o. tuticorin v. t.s.d. nadar, hegde j ': 1.0}\n",
            "          Brij Bihari Lal Agarwal vs High Court Of Madhya Pradesh & Ors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {'c] union of india v. col ': 0.8, 'state of uttar pradesh v. chandra mohan nigam ': 0.8, 'e] r. l. butail v. union of india ': 0.8, 'gurdial singh fiji v. state of punjab ': 1.0, 'union of india v. m. r. reddy ': 1.0, 'in state of uttar pradesh v. chandra mohan nigam ': 1.0, 'gurdial singh fijji v. state of punjab ': 1.0, 'union of india v. m. e. reddy ': 1.0}\n",
            "                                Swadeshi Cotton Mills vs Union Of India {'keshav mills co. ltd. v. union of india ': 0.8, 'kamla prasad khetan v. union of india ': 0.8, 'maneka gandhi v. union of india ': 0.8, 'sukhdev singh & ors. v. bhagatram sardar singh ': 0.8, 'a. k. kraipak v. union of india ': 0.8, 'nawabkhan abbaskhan v. state of gujarat ': 0.8, 'ambalal m. shah v. hathi singh manufacturing co. ltd ': 0.8, 's. l. kapoor v. jagmohan & ors ': 0.8, 'keshav mills company ltd. v. union of india ': 0.8, 'dosabhai ratanshah keravale v. state of gujarat ': 1.0, 'mohinder singh gill v. election commissioner of india ': 1.0, 'wiseman v. borneman ': 1.0, 'mohinder singh gill v. chief election commissioner ': 1.0, 'union of india v. col ': 1.0, 'madhav hayawadanrao hoskot v. maharashtra ': 1.0, 'vijay kumar mundhra v. union of india ': 1.0, 'corporation of calcutta v. calcutta tramways ': 1.0, 'see union of india v. col ': 1.0, 'in wiseman v. borneman ': 1.0, 'in narayan govind gavate v. state of maharashtra & ors ': 1.0, 'dora phalauli v. state of punjab & ors ': 1.0, 'state of punjab v. gurdial singh ': 1.0, 'ambalal m. shah v. hathi singh manufacturing co ltd ': 1.0, 'shri ambalal m. shah & anr. v. hathi singh manufacturing co. ltd ': 1.0, 'john v. rees & ors ': 1.0}\n",
            "         Lalappa Lingappa & Ors vs Laxmi Vishnu Textile Mills Ltd., ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'delhi cloth and general mills co. v. its workmen ': 1.0, 'ltd., calcutta v. its workmen ': 1.0, 'explanation i. in delhi cloth and general mills co. v. its workmen ': 1.0}\n",
            "Rameshchandra Kachardas Porwal & ... vs State Of Maharashtra & Ors. Etc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {'tulsipur sugar co. v. notified area committee ': 0.8, 'bapubhai ratanchand shah v. the state of bombay ': 0.8, 'in kewal krishan puri & anr. v. state of punjab & ors ': 1.0, 'in mohammadbhai khudabux chhippa & anr. v. the state of gujarat & anr ': 1.0, 'ram chandra kailash kumar & co. & ors. v. state of u.p. & anr ': 1.0}\n",
            "                            J. D. Shrivastava vs State Of M.P. & Others                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'d.ramaswami v. state of tamil nadu ': 0.8, 'b-c] union of india v. col ': 0.8, 'union of india v. m.e. reddy & anr ': 0.8, 'swami saran saksena v. state of u.p ': 1.0, 'brij bihari lal agarwal v. high ': 1.0, 'see union of india v. col ': 1.0, 'state of u.p., baldev raj chadha v. union of india ': 1.0, 'brij bihari lal agawral v. high ': 1.0}\n",
            "                                   M.P. Pradhan vs Union Of India & Ors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
            "                                      Prithipal Singh vs Union Of India                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {'des raj and ors. v. state of punjab and ors ': 1.0}\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    Section 420 - Cheating, Section 34 - Common Intent\n",
            "\n",
            "    **Prayer Clause:**\n",
            "    The petitioner prays for appropriate relief, including compensation.\n",
            "\n",
            "    \n",
            "**Interim Relief Sought:**\n",
            "Stay order on eviction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "Ebb7HjUqfxKW",
        "outputId": "efc8dc09-0f21-4156-8c60-50006ee2119b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.59.5\n",
            "    Uninstalling openai-1.59.5:\n",
            "      Successfully uninstalled openai-1.59.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "exa-py 1.7.1 requires openai>=1.10.0, but you have openai 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "d0866deaf8e94ff9947c915d2cc2170e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Function to run `openai migrate`\n",
        "def run_openai_migrate():\n",
        "    \"\"\"\n",
        "    Runs the `openai migrate` command to upgrade the codebase.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"openai\", \"migrate\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        print(\"Migration Output:\\n\", result.stdout)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Migration Failed:\\n\", e.stderr)\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Step 2: Function to fetch IPC section details\n",
        "def fetch_ipc_section_details(section):\n",
        "    \"\"\"\n",
        "    Fetch details for an IPC section using the OpenAI API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal expert.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Provide details about IPC Section {section}.\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for Section {section}: {e}\")\n",
        "        return f\"Description unavailable for Section {section}.\"\n",
        "\n",
        "# Step 3: Map IPC sections to cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    \"\"\"\n",
        "    Map IPC sections to cases in the dataset.\n",
        "    \"\"\"\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        \"\"\"\n",
        "        Map issues to IPC sections and fetch details for each section.\n",
        "        \"\"\"\n",
        "        if not isinstance(issue_text, str):\n",
        "            return \"No relevant IPC sections found.\"\n",
        "\n",
        "        # Example IPC sections for mapping\n",
        "        ipc_sections = [\"302\", \"307\", \"420\"]  # Add more as needed\n",
        "        ipc_details = {}\n",
        "        for section in ipc_sections:\n",
        "            if section in issue_text:\n",
        "                ipc_details[f\"Section {section}\"] = fetch_ipc_section_details(section)\n",
        "        return ipc_details if ipc_details else \"No relevant IPC sections found.\"\n",
        "\n",
        "    # Apply the mapping to each row in the dataset\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Step 4: Generate a petition from IPC mappings\n",
        "def generate_petition(ipc_mappings):\n",
        "    \"\"\"\n",
        "    Generate petitions based on IPC mappings.\n",
        "    \"\"\"\n",
        "    petitions = []\n",
        "    for _, row in ipc_mappings.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "\n",
        "        # Generate petition text\n",
        "        petition_text = f\"Petition for {case_title}:\\n\\n\"\n",
        "        if isinstance(ipc_details, dict):\n",
        "            for section, description in ipc_details.items():\n",
        "                petition_text += f\"{section}: {description}\\n\"\n",
        "        else:\n",
        "            petition_text += \"No IPC details available for this case.\\n\"\n",
        "\n",
        "        petitions.append({\"case_title\": case_title, \"petition\": petition_text})\n",
        "    return petitions\n",
        "\n",
        "# Step 5: Main Execution\n",
        "def main():\n",
        "    # Run `openai migrate` before proceeding\n",
        "    migration_success = run_openai_migrate()\n",
        "    if not migration_success:\n",
        "        print(\"Migration failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"Section 302, Section 307\", \"Section 420\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate petitions\n",
        "    petitions = generate_petition(processed_df)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nProcessed Dataset with IPC Mappings:\")\n",
        "    print(processed_df)\n",
        "\n",
        "    print(\"\\nGenerated Petitions:\")\n",
        "    for petition in petitions:\n",
        "        print(f\"Case Title: {petition['case_title']}\\n\")\n",
        "        print(f\"Petition:\\n{petition['petition']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_BaB7GhifEl",
        "outputId": "2441fb7d-0fdb-48a2-94e0-38ab68b38b88"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Migration Failed:\n",
            " usage: openai [-h] [-V] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION]\n",
            "              {api,tools,wandb} ...\n",
            "openai: error: argument {api,tools,wandb}: invalid choice: 'migrate' (choose from 'api', 'tools', 'wandb')\n",
            "\n",
            "Migration failed. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Fetch IPC Section Details\n",
        "def fetch_ipc_section_details(section):\n",
        "    \"\"\"\n",
        "    Fetch details for an IPC section using the OpenAI API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal expert.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Provide details about IPC Section {section}.\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for Section {section}: {e}\")\n",
        "        return f\"Description unavailable for Section {section}.\"\n",
        "\n",
        "# Step 2: Map IPC Sections to Cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    \"\"\"\n",
        "    Map IPC sections to cases in the dataset.\n",
        "    \"\"\"\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        \"\"\"\n",
        "        Map issues to IPC sections and fetch details for each section.\n",
        "        \"\"\"\n",
        "        if not isinstance(issue_text, str):\n",
        "            return \"No relevant IPC sections found.\"\n",
        "\n",
        "        # Example IPC sections for mapping\n",
        "        ipc_sections = [\"302\", \"307\", \"420\"]  # Add more as needed\n",
        "        ipc_details = {}\n",
        "        for section in ipc_sections:\n",
        "            if section in issue_text:\n",
        "                ipc_details[f\"Section {section}\"] = fetch_ipc_section_details(section)\n",
        "        return ipc_details if ipc_details else \"No relevant IPC sections found.\"\n",
        "\n",
        "    # Apply the mapping to each row in the dataset\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Step 3: Generate a petition from IPC mappings\n",
        "def generate_petition(ipc_mappings):\n",
        "    \"\"\"\n",
        "    Generate petitions based on IPC mappings.\n",
        "    \"\"\"\n",
        "    petitions = []\n",
        "    for _, row in ipc_mappings.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "\n",
        "        # Generate petition text\n",
        "        petition_text = f\"Petition for {case_title}:\\n\\n\"\n",
        "        if isinstance(ipc_details, dict):\n",
        "            for section, description in ipc_details.items():\n",
        "                petition_text += f\"{section}: {description}\\n\"\n",
        "        else:\n",
        "            petition_text += \"No IPC details available for this case.\\n\"\n",
        "\n",
        "        petitions.append({\"case_title\": case_title, \"petition\": petition_text})\n",
        "    return petitions\n",
        "\n",
        "# Step 4: Main Execution\n",
        "def main():\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"Section 302, Section 307\", \"Section 420\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df.columns = df.columns.str.strip().str.lower()  # Standardize column names\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate petitions\n",
        "    petitions = generate_petition(processed_df)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nProcessed Dataset with IPC Mappings:\")\n",
        "    print(processed_df)\n",
        "\n",
        "    print(\"\\nGenerated Petitions:\")\n",
        "    for petition in petitions:\n",
        "        print(f\"Case Title: {petition['case_title']}\\n\")\n",
        "        print(f\"Petition:\\n{petition['petition']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3IaayLHitVg",
        "outputId": "7b430b90-c74f-47f0-e31c-9fc35f7f9a31"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching details for Section 302: The model `gpt-4` does not exist or you do not have access to it.\n",
            "Error fetching details for Section 307: The model `gpt-4` does not exist or you do not have access to it.\n",
            "Error fetching details for Section 420: The model `gpt-4` does not exist or you do not have access to it.\n",
            "\n",
            "Processed Dataset with IPC Mappings:\n",
            "  case_title                    issues  \\\n",
            "0     Case A  Section 302, Section 307   \n",
            "1     Case B               Section 420   \n",
            "\n",
            "                                        ipc_mappings  \n",
            "0  {'Section 302': 'Description unavailable for S...  \n",
            "1  {'Section 420': 'Description unavailable for S...  \n",
            "\n",
            "Generated Petitions:\n",
            "Case Title: Case A\n",
            "\n",
            "Petition:\n",
            "Petition for Case A:\n",
            "\n",
            "Section 302: Description unavailable for Section 302.\n",
            "Section 307: Description unavailable for Section 307.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Case Title: Case B\n",
            "\n",
            "Petition:\n",
            "Petition for Case B:\n",
            "\n",
            "Section 420: Description unavailable for Section 420.\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace <YOUR_API_KEY> with your actual OpenAI API key\n",
        "\n",
        "# Step 1: Fetch IPC Section Details\n",
        "def fetch_ipc_section_details(section):\n",
        "    \"\"\"\n",
        "    Fetch details for an IPC section using the OpenAI API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal expert.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Provide details about IPC Section {section}.\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for Section {section}: {e}\")\n",
        "        return f\"Description unavailable for Section {section}.\"\n",
        "\n",
        "# Step 2: Map IPC Sections to Cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    \"\"\"\n",
        "    Map IPC sections to cases in the dataset.\n",
        "    \"\"\"\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        \"\"\"\n",
        "        Map issues to IPC sections and fetch details for each section.\n",
        "        \"\"\"\n",
        "        if not isinstance(issue_text, str):\n",
        "            return \"No relevant IPC sections found.\"\n",
        "\n",
        "        # Example IPC sections for mapping\n",
        "        ipc_sections = [\"302\", \"307\", \"420\"]  # Add more as needed\n",
        "        ipc_details = {}\n",
        "        for section in ipc_sections:\n",
        "            if section in issue_text:\n",
        "                ipc_details[f\"Section {section}\"] = fetch_ipc_section_details(section)\n",
        "        return ipc_details if ipc_details else \"No relevant IPC sections found.\"\n",
        "\n",
        "    # Apply the mapping to each row in the dataset\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Step 3: Generate a petition from IPC mappings\n",
        "def generate_petition(ipc_mappings):\n",
        "    \"\"\"\n",
        "    Generate petitions based on IPC mappings.\n",
        "    \"\"\"\n",
        "    petitions = []\n",
        "    for _, row in ipc_mappings.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "\n",
        "        # Generate petition text\n",
        "        petition_text = f\"Petition for {case_title}:\\n\\n\"\n",
        "        if isinstance(ipc_details, dict):\n",
        "            for section, description in ipc_details.items():\n",
        "                petition_text += f\"{section}: {description}\\n\"\n",
        "        else:\n",
        "            petition_text += \"No IPC details available for this case.\\n\"\n",
        "\n",
        "        petitions.append({\"case_title\": case_title, \"petition\": petition_text})\n",
        "    return petitions\n",
        "\n",
        "# Step 4: Main Execution\n",
        "def main():\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"Section 302, Section 307\", \"Section 420\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df.columns = df.columns.str.strip().str.lower()  # Standardize column names\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate petitions\n",
        "    petitions = generate_petition(processed_df)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nProcessed Dataset with IPC Mappings:\")\n",
        "    print(processed_df)\n",
        "\n",
        "    print(\"\\nGenerated Petitions:\")\n",
        "    for petition in petitions:\n",
        "        print(f\"Case Title: {petition['case_title']}\\n\")\n",
        "        print(f\"Petition:\\n{petition['petition']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4k3p5cxjcPX",
        "outputId": "6f2447ec-59d7-4ef7-f7d9-983a58954b4f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching details for Section 302: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "Error fetching details for Section 307: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "Error fetching details for Section 420: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "\n",
            "Processed Dataset with IPC Mappings:\n",
            "  case_title                    issues  \\\n",
            "0     Case A  Section 302, Section 307   \n",
            "1     Case B               Section 420   \n",
            "\n",
            "                                        ipc_mappings  \n",
            "0  {'Section 302': 'Description unavailable for S...  \n",
            "1  {'Section 420': 'Description unavailable for S...  \n",
            "\n",
            "Generated Petitions:\n",
            "Case Title: Case A\n",
            "\n",
            "Petition:\n",
            "Petition for Case A:\n",
            "\n",
            "Section 302: Description unavailable for Section 302.\n",
            "Section 307: Description unavailable for Section 307.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Case Title: Case B\n",
            "\n",
            "Petition:\n",
            "Petition for Case B:\n",
            "\n",
            "Section 420: Description unavailable for Section 420.\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace with your actual OpenAI API key\n",
        "\n",
        "# Function to fetch IPC section details\n",
        "def fetch_ipc_section_details(section):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal expert.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Provide a detailed explanation and implications of IPC Section {section}.\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for Section {section}: {e}\")\n",
        "        return f\"Description unavailable for Section {section}.\"\n",
        "\n",
        "# Function to map IPC sections to cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        if not isinstance(issue_text, str):\n",
        "            return {}\n",
        "        ipc_sections = [int(s.strip()) for s in issue_text.split(',') if s.strip().isdigit()]\n",
        "        ipc_details = {f\"Section {section}\": fetch_ipc_section_details(section) for section in ipc_sections}\n",
        "        return ipc_details\n",
        "\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Function to draft a detailed petition\n",
        "def draft_petition(case_title, summary, ipc_details, prayer, interim_relief=None):\n",
        "    petition = f\"\"\"\n",
        "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
        "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
        "\n",
        "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
        "\n",
        "    TITLE: {case_title}\n",
        "\n",
        "    **Facts of the Case:**\n",
        "    {summary}\n",
        "\n",
        "    **Applicable Legal Provisions:**\n",
        "    \"\"\"\n",
        "    for section, description in ipc_details.items():\n",
        "        petition += f\"\\n- {section}: {description}\\n\"\n",
        "\n",
        "    petition += f\"\\n**Prayer Clause:**\\n{prayer}\\n\"\n",
        "\n",
        "    if interim_relief:\n",
        "        petition += f\"\\n**Interim Relief Sought:**\\n{interim_relief}\"\n",
        "\n",
        "    return petition\n",
        "\n",
        "# Function to generate petitions for all cases\n",
        "def generate_petitions(ipc_mappings):\n",
        "    petitions = []\n",
        "    for _, row in ipc_mappings.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "        summary = f\"This petition concerns the issues arising from the application of IPC sections related to the case titled '{case_title}'.\"\n",
        "        prayer = \"The petitioner prays for appropriate relief, including justice and compensation.\"\n",
        "        interim_relief = \"An immediate stay on the proceedings impacting the petitioner.\"\n",
        "\n",
        "        petition_text = draft_petition(case_title, summary, ipc_details, prayer, interim_relief)\n",
        "        petitions.append({\"case_title\": case_title, \"petition\": petition_text})\n",
        "    return petitions\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"302, 307\", \"420\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate detailed petitions\n",
        "    petitions = generate_petitions(processed_df)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nProcessed Dataset with IPC Mappings:\")\n",
        "    print(processed_df)\n",
        "\n",
        "    print(\"\\nGenerated Petitions:\")\n",
        "    for petition in petitions:\n",
        "        print(f\"Case Title: {petition['case_title']}\\n\")\n",
        "        print(f\"Petition:\\n{petition['petition']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyaKSZufj9TX",
        "outputId": "3e715d23-9576-4be7-c44b-5c23d990ab65"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching details for Section 302: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "Error fetching details for Section 307: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "Error fetching details for Section 420: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "\n",
            "Processed Dataset with IPC Mappings:\n",
            "  case_title    issues                                       ipc_mappings\n",
            "0     Case A  302, 307  {'Section 302': 'Description unavailable for S...\n",
            "1     Case B       420  {'Section 420': 'Description unavailable for S...\n",
            "\n",
            "Generated Petitions:\n",
            "Case Title: Case A\n",
            "\n",
            "Petition:\n",
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Case A\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    This petition concerns the issues arising from the application of IPC sections related to the case titled 'Case A'.\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    \n",
            "- Section 302: Description unavailable for Section 302.\n",
            "\n",
            "- Section 307: Description unavailable for Section 307.\n",
            "\n",
            "**Prayer Clause:**\n",
            "The petitioner prays for appropriate relief, including justice and compensation.\n",
            "\n",
            "**Interim Relief Sought:**\n",
            "An immediate stay on the proceedings impacting the petitioner.\n",
            "\n",
            "================================================================================\n",
            "Case Title: Case B\n",
            "\n",
            "Petition:\n",
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Case B\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    This petition concerns the issues arising from the application of IPC sections related to the case titled 'Case B'.\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    \n",
            "- Section 420: Description unavailable for Section 420.\n",
            "\n",
            "**Prayer Clause:**\n",
            "The petitioner prays for appropriate relief, including justice and compensation.\n",
            "\n",
            "**Interim Relief Sought:**\n",
            "An immediate stay on the proceedings impacting the petitioner.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace with your actual OpenAI API key\n",
        "\n",
        "# Function to fetch IPC section details\n",
        "def fetch_ipc_section_details(section):\n",
        "    \"\"\"\n",
        "    Fetch detailed information about an IPC section using the OpenAI API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal expert.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Provide a comprehensive explanation, historical context, and implications of IPC Section {section}.\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for Section {section}: {e}\")\n",
        "        return f\"Description unavailable for Section {section}.\"\n",
        "\n",
        "# Function to map IPC sections to cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    \"\"\"\n",
        "    Map IPC sections mentioned in issues to detailed descriptions using OpenAI.\n",
        "    \"\"\"\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        if not isinstance(issue_text, str):\n",
        "            return {}\n",
        "        ipc_sections = [int(s.strip()) for s in issue_text.split(',') if s.strip().isdigit()]\n",
        "        ipc_details = {f\"Section {section}\": fetch_ipc_section_details(section) for section in ipc_sections}\n",
        "        return ipc_details\n",
        "\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Function to draft a detailed petition\n",
        "def draft_petition(case_title, summary, ipc_details, prayer, interim_relief=None):\n",
        "    \"\"\"\n",
        "    Generate a detailed, court-ready petition.\n",
        "    \"\"\"\n",
        "    petition = f\"\"\"\n",
        "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
        "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
        "\n",
        "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
        "\n",
        "    **CASE TITLE:** {case_title}\n",
        "\n",
        "    **FACTS OF THE CASE:**\n",
        "    1. The petitioner is aggrieved by the actions/inactions that have led to this legal recourse.\n",
        "    2. The primary issues revolve around the following legal and factual matters:\n",
        "       - {summary}\n",
        "    3. The actions of the respondent(s) are in direct violation of the fundamental rights guaranteed under the Constitution of India.\n",
        "\n",
        "    **APPLICABLE LEGAL PROVISIONS:**\n",
        "    The following sections of the Indian Penal Code (IPC) are directly relevant to this case:\n",
        "    \"\"\"\n",
        "    for section, description in ipc_details.items():\n",
        "        petition += f\"\\n- **{section}:**\\n  {description}\\n\"\n",
        "\n",
        "    petition += f\"\"\"\n",
        "    **PRAYER CLAUSE:**\n",
        "    The petitioner respectfully prays for the following reliefs:\n",
        "    1. Issue appropriate writ(s) under Article 32/226 of the Constitution directing the respondent(s) to take remedial actions as per the law.\n",
        "    2. Grant monetary compensation or reparations to the petitioner for the irreparable damage caused.\n",
        "    3. Pass any other order(s) as deemed fit and appropriate by this Hon'ble Court.\n",
        "\n",
        "    **INTERIM RELIEF (IF ANY):**\n",
        "    \"\"\"\n",
        "    if interim_relief:\n",
        "        petition += f\"  - {interim_relief}\\n\"\n",
        "    else:\n",
        "        petition += \"  - No interim relief sought at this stage.\\n\"\n",
        "\n",
        "    petition += \"\"\"\n",
        "    **CITATIONS AND REFERENCES:**\n",
        "    1. Previous judicial precedents and relevant case laws will be submitted during the hearing as per the court's directions.\n",
        "    2. Supporting legal interpretations and documentation are annexed herewith.\n",
        "    \"\"\"\n",
        "\n",
        "    return petition\n",
        "\n",
        "# Function to generate petitions for all cases\n",
        "def generate_petitions(ipc_mappings):\n",
        "    \"\"\"\n",
        "    Generate petitions for each case based on IPC mappings.\n",
        "    \"\"\"\n",
        "    petitions = []\n",
        "    for _, row in ipc_mappings.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "        summary = f\"This case pertains to violations involving IPC sections and legal provisions relevant to the petitioner's grievances.\"\n",
        "        prayer = \"Grant relief as deemed appropriate, including compensation and necessary directives to the respondents.\"\n",
        "        interim_relief = \"Grant a stay on the disputed actions or proceedings impacting the petitioner.\"\n",
        "\n",
        "        petition_text = draft_petition(case_title, summary, ipc_details, prayer, interim_relief)\n",
        "        petitions.append({\"case_title\": case_title, \"petition\": petition_text})\n",
        "    return petitions\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"302, 307\", \"420\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate detailed petitions\n",
        "    petitions = generate_petitions(processed_df)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nProcessed Dataset with IPC Mappings:\")\n",
        "    print(processed_df)\n",
        "\n",
        "    print(\"\\nGenerated Petitions:\")\n",
        "    for petition in petitions:\n",
        "        print(f\"Case Title: {petition['case_title']}\\n\")\n",
        "        print(f\"Petition:\\n{petition['petition']}\\n\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-cIFVXylswZ",
        "outputId": "a3a42585-e862-4099-9fd6-ae0dbbf6cf8d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching details for Section 302: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "Error fetching details for Section 307: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "Error fetching details for Section 420: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "\n",
            "Processed Dataset with IPC Mappings:\n",
            "  case_title    issues                                       ipc_mappings\n",
            "0     Case A  302, 307  {'Section 302': 'Description unavailable for S...\n",
            "1     Case B       420  {'Section 420': 'Description unavailable for S...\n",
            "\n",
            "Generated Petitions:\n",
            "Case Title: Case A\n",
            "\n",
            "Petition:\n",
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "\n",
            "    **CASE TITLE:** Case A\n",
            "\n",
            "    **FACTS OF THE CASE:**\n",
            "    1. The petitioner is aggrieved by the actions/inactions that have led to this legal recourse.\n",
            "    2. The primary issues revolve around the following legal and factual matters:\n",
            "       - This case pertains to violations involving IPC sections and legal provisions relevant to the petitioner's grievances.\n",
            "    3. The actions of the respondent(s) are in direct violation of the fundamental rights guaranteed under the Constitution of India.\n",
            "\n",
            "    **APPLICABLE LEGAL PROVISIONS:**\n",
            "    The following sections of the Indian Penal Code (IPC) are directly relevant to this case:\n",
            "    \n",
            "- **Section 302:**\n",
            "  Description unavailable for Section 302.\n",
            "\n",
            "- **Section 307:**\n",
            "  Description unavailable for Section 307.\n",
            "\n",
            "    **PRAYER CLAUSE:**\n",
            "    The petitioner respectfully prays for the following reliefs:\n",
            "    1. Issue appropriate writ(s) under Article 32/226 of the Constitution directing the respondent(s) to take remedial actions as per the law.\n",
            "    2. Grant monetary compensation or reparations to the petitioner for the irreparable damage caused.\n",
            "    3. Pass any other order(s) as deemed fit and appropriate by this Hon'ble Court.\n",
            "\n",
            "    **INTERIM RELIEF (IF ANY):**\n",
            "      - Grant a stay on the disputed actions or proceedings impacting the petitioner.\n",
            "\n",
            "    **CITATIONS AND REFERENCES:**\n",
            "    1. Previous judicial precedents and relevant case laws will be submitted during the hearing as per the court's directions.\n",
            "    2. Supporting legal interpretations and documentation are annexed herewith.\n",
            "    \n",
            "\n",
            "================================================================================\n",
            "Case Title: Case B\n",
            "\n",
            "Petition:\n",
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "\n",
            "    **CASE TITLE:** Case B\n",
            "\n",
            "    **FACTS OF THE CASE:**\n",
            "    1. The petitioner is aggrieved by the actions/inactions that have led to this legal recourse.\n",
            "    2. The primary issues revolve around the following legal and factual matters:\n",
            "       - This case pertains to violations involving IPC sections and legal provisions relevant to the petitioner's grievances.\n",
            "    3. The actions of the respondent(s) are in direct violation of the fundamental rights guaranteed under the Constitution of India.\n",
            "\n",
            "    **APPLICABLE LEGAL PROVISIONS:**\n",
            "    The following sections of the Indian Penal Code (IPC) are directly relevant to this case:\n",
            "    \n",
            "- **Section 420:**\n",
            "  Description unavailable for Section 420.\n",
            "\n",
            "    **PRAYER CLAUSE:**\n",
            "    The petitioner respectfully prays for the following reliefs:\n",
            "    1. Issue appropriate writ(s) under Article 32/226 of the Constitution directing the respondent(s) to take remedial actions as per the law.\n",
            "    2. Grant monetary compensation or reparations to the petitioner for the irreparable damage caused.\n",
            "    3. Pass any other order(s) as deemed fit and appropriate by this Hon'ble Court.\n",
            "\n",
            "    **INTERIM RELIEF (IF ANY):**\n",
            "      - Grant a stay on the disputed actions or proceedings impacting the petitioner.\n",
            "\n",
            "    **CITATIONS AND REFERENCES:**\n",
            "    1. Previous judicial precedents and relevant case laws will be submitted during the hearing as per the court's directions.\n",
            "    2. Supporting legal interpretations and documentation are annexed herewith.\n",
            "    \n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Fetch IPC Section Details Locally\n",
        "ipc_section_dict = {\n",
        "    140: {\n",
        "        \"description\": \"If someone who is not a military member wears a uniform or carries something resembling a military uniform to deceive others into believing they are a soldier, sailor, or airman, they can be punished with up to three months in jail, a fine of up to five hundred rupees, or both.\",\n",
        "        \"offense\": \"Wearing the dress or carrying any token used by a soldier, sailor or airman with intent that it may be believed that he is such a soldier, sailor or airman.\",\n",
        "        \"punishment\": \"3 Months or Fine or Both.\"\n",
        "    },\n",
        "    127: {\n",
        "        \"description\": \"If someone receives property knowing it was taken during the commission of certain offenses (mentioned in sections 125 and 126), they can be punished with imprisonment of up to seven years, fined, and the property can be forfeited.\",\n",
        "        \"offense\": \"Receiving property taken by war or depredation mentioned in sections 125 and 126.\",\n",
        "        \"punishment\": \"7 Years + Fine + forfeiture of property.\"\n",
        "    },\n",
        "    # Add more sections here...\n",
        "}\n",
        "\n",
        "# Step 2: Map IPC Sections to Cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    \"\"\"\n",
        "    Map IPC sections to cases in the dataset using local data.\n",
        "    \"\"\"\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        \"\"\"\n",
        "        Map issues to IPC sections and fetch details for each section.\n",
        "        \"\"\"\n",
        "        if not isinstance(issue_text, str):\n",
        "            return \"No relevant IPC sections found.\"\n",
        "\n",
        "        # Example IPC sections for mapping\n",
        "        ipc_sections = [int(s.strip()) for s in issue_text.split(',') if s.strip().isdigit()]\n",
        "        ipc_details = {}\n",
        "        for section in ipc_sections:\n",
        "            if section in ipc_section_dict:\n",
        "                ipc_details[f\"Section {section}\"] = ipc_section_dict[section]\n",
        "        return ipc_details if ipc_details else \"No relevant IPC sections found.\"\n",
        "\n",
        "    # Apply the mapping to each row in the dataset\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Step 3: Draft a Petition\n",
        "def draft_petition(case_title, summary, ipc_details, prayer, interim_relief=None):\n",
        "    petition = f\"\"\"\n",
        "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
        "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
        "\n",
        "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
        "\n",
        "    TITLE: {case_title}\n",
        "\n",
        "    **Facts of the Case:**\n",
        "    {summary}\n",
        "\n",
        "    **Applicable Legal Provisions:**\n",
        "    \"\"\"\n",
        "\n",
        "    # Add IPC sections and their details to the petition\n",
        "    for section, details in ipc_details.items():\n",
        "        petition += f\"\\n{section} - {details.get('offense', 'Offense details not available')}\"\n",
        "        petition += f\"\\nPunishment: {details.get('punishment', 'Punishment details not available')}\"\n",
        "        petition += f\"\\nCategory: {details.get('category', 'Category details not available')}\\n\"\n",
        "\n",
        "    petition += f\"\\n**Prayer Clause:**\\n{prayer}\"\n",
        "\n",
        "    if interim_relief:\n",
        "        petition += f\"\\n\\n**Interim Relief Sought:**\\n{interim_relief}\"\n",
        "\n",
        "    return petition  # Ensure this is within the function\n",
        "\n",
        "\n",
        "# Step 4: Main Execution\n",
        "def main():\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"140, 127\", \"127\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df.columns = df.columns.str.strip().str.lower()  # Standardize column names\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate petitions\n",
        "    for _, row in processed_df.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "\n",
        "        # Draft a petition for the case\n",
        "        petition = draft_petition(\n",
        "            case_title=case_title,\n",
        "            summary=\"Detailed summary of the case goes here.\",\n",
        "            ipc_details=ipc_details,\n",
        "            prayer=\"The petitioner prays for appropriate relief, including compensation.\",\n",
        "            interim_relief=\"Stay order on eviction.\"\n",
        "        )\n",
        "        print(petition)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_WUnjNrny0P",
        "outputId": "47cea49b-440f-41f7-d632-3d7106bd83fd"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Case A\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    Detailed summary of the case goes here.\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    \n",
            "Section 140 - Wearing the dress or carrying any token used by a soldier, sailor or airman with intent that it may be believed that he is such a soldier, sailor or airman.\n",
            "Punishment: 3 Months or Fine or Both.\n",
            "Category: Category details not available\n",
            "\n",
            "Section 127 - Receiving property taken by war or depredation mentioned in sections 125 and 126.\n",
            "Punishment: 7 Years + Fine + forfeiture of property.\n",
            "Category: Category details not available\n",
            "\n",
            "**Prayer Clause:**\n",
            "The petitioner prays for appropriate relief, including compensation.\n",
            "\n",
            "**Interim Relief Sought:**\n",
            "Stay order on eviction.\n",
            "================================================================================\n",
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Case B\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    Detailed summary of the case goes here.\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    \n",
            "Section 127 - Receiving property taken by war or depredation mentioned in sections 125 and 126.\n",
            "Punishment: 7 Years + Fine + forfeiture of property.\n",
            "Category: Category details not available\n",
            "\n",
            "**Prayer Clause:**\n",
            "The petitioner prays for appropriate relief, including compensation.\n",
            "\n",
            "**Interim Relief Sought:**\n",
            "Stay order on eviction.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Local IPC Section Dictionary with Details\n",
        "ipc_section_dict = {\n",
        "    140: {\n",
        "        \"description\": \"If someone who is not a military member wears a uniform or carries something resembling a military uniform to deceive others into believing they are a soldier, sailor, or airman, they can be punished with up to three months in jail, a fine of up to five hundred rupees, or both.\",\n",
        "        \"offense\": \"Wearing the dress or carrying any token used by a soldier, sailor or airman with intent that it may be believed that he is such a soldier, sailor or airman.\",\n",
        "        \"punishment\": \"3 Months or Fine or Both.\"\n",
        "    },\n",
        "    127: {\n",
        "        \"description\": \"If someone receives property knowing it was taken during the commission of certain offenses (mentioned in sections 125 and 126), they can be punished with imprisonment of up to seven years, fined, and the property can be forfeited.\",\n",
        "        \"offense\": \"Receiving property taken by war or depredation mentioned in sections 125 and 126.\",\n",
        "        \"punishment\": \"7 Years + Fine + forfeiture of property.\"\n",
        "    },\n",
        "    # Add more sections here with structured details\n",
        "}\n",
        "\n",
        "# Step 2: Map IPC Sections to Cases\n",
        "def map_ipc_sections(dataset, section_column='issues'):\n",
        "    \"\"\"\n",
        "    Process the dataset to map IPC sections to corresponding legal details.\n",
        "\n",
        "    1. **Inputs**:\n",
        "        - `dataset`: A pandas DataFrame containing case titles and IPC sections.\n",
        "        - `section_column`: The column in the dataset where IPC sections are listed.\n",
        "\n",
        "    2. **Processing**:\n",
        "        - Split the IPC section numbers in the `issues` column (e.g., \"140, 127\").\n",
        "        - Match each section with its corresponding details from `ipc_section_dict`.\n",
        "\n",
        "    3. **Outputs**:\n",
        "        - Adds a new column `ipc_mappings` to the dataset, containing detailed IPC mappings.\n",
        "    \"\"\"\n",
        "    def map_issues_to_ipc(issue_text):\n",
        "        \"\"\"\n",
        "        Map issues (text) to detailed IPC section details.\n",
        "\n",
        "        **Details**:\n",
        "        - Extract IPC section numbers from text.\n",
        "        - Retrieve matching details from `ipc_section_dict`.\n",
        "        \"\"\"\n",
        "        if not isinstance(issue_text, str):\n",
        "            return \"No relevant IPC sections found.\"\n",
        "\n",
        "        # Extract numeric section IDs and map them to the IPC dictionary\n",
        "        ipc_sections = [int(s.strip()) for s in issue_text.split(',') if s.strip().isdigit()]\n",
        "        ipc_details = {}\n",
        "        for section in ipc_sections:\n",
        "            if section in ipc_section_dict:\n",
        "                ipc_details[f\"Section {section}\"] = ipc_section_dict[section]\n",
        "        return ipc_details if ipc_details else \"No relevant IPC sections found.\"\n",
        "\n",
        "    # Apply mapping to each row in the dataset\n",
        "    dataset['ipc_mappings'] = dataset[section_column].apply(map_issues_to_ipc)\n",
        "    return dataset\n",
        "\n",
        "# Step 3: Draft a Petition\n",
        "def draft_petition(case_title, summary, ipc_details, prayer, interim_relief=None):\n",
        "    \"\"\"\n",
        "    Generate a detailed legal petition for court submission.\n",
        "\n",
        "    **Parameters**:\n",
        "    1. `case_title`: The title of the case.\n",
        "    2. `summary`: A detailed summary of the facts of the case.\n",
        "    3. `ipc_details`: A dictionary containing IPC section details for the case.\n",
        "    4. `prayer`: The relief sought by the petitioner.\n",
        "    5. `interim_relief`: Optional interim relief requested by the petitioner.\n",
        "\n",
        "    **Structure**:\n",
        "    - **Header**: Supreme Court jurisdiction and petition type.\n",
        "    - **Case Facts**: Detailed description of the case.\n",
        "    - **IPC Details**: Point-by-point breakdown of offenses, punishments, and descriptions.\n",
        "    - **Prayer Clause**: The relief sought.\n",
        "    - **Interim Relief (Optional)**: Any temporary relief required.\n",
        "    \"\"\"\n",
        "    petition = f\"\"\"\n",
        "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
        "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
        "\n",
        "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
        "\n",
        "    TITLE: {case_title}\n",
        "\n",
        "    **Facts of the Case:**\n",
        "    {summary}\n",
        "\n",
        "    **Applicable Legal Provisions:**\n",
        "    \"\"\"\n",
        "\n",
        "    # Add IPC sections and their details to the petition\n",
        "    for section, details in ipc_details.items():\n",
        "        petition += f\"\\n- {section}:\"\n",
        "        petition += f\"\\n  - **Offense**: {details.get('offense', 'Offense details not available')}\"\n",
        "        petition += f\"\\n  - **Punishment**: {details.get('punishment', 'Punishment details not available')}\"\n",
        "        petition += f\"\\n  - **Description**: {details.get('description', 'Description not available')}\"\n",
        "\n",
        "    petition += f\"\\n\\n**Prayer Clause:**\\n- {prayer}\"\n",
        "\n",
        "    if interim_relief:\n",
        "        petition += f\"\\n\\n**Interim Relief Sought:**\\n- {interim_relief}\"\n",
        "\n",
        "    return petition\n",
        "\n",
        "# Step 4: Main Execution\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to process cases and generate detailed legal petitions.\n",
        "\n",
        "    **Workflow**:\n",
        "    1. Create a sample dataset with case titles and associated IPC sections.\n",
        "    2. Map IPC sections to their legal details using `map_ipc_sections`.\n",
        "    3. Generate detailed petitions for each case using `draft_petition`.\n",
        "    \"\"\"\n",
        "    # Sample dataset (replace with actual dataset loading)\n",
        "    data = {\n",
        "        \"case_title\": [\"Case A\", \"Case B\"],\n",
        "        \"issues\": [\"140, 127\", \"127\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    df.columns = df.columns.str.strip().str.lower()  # Standardize column names\n",
        "\n",
        "    # Map IPC sections to cases\n",
        "    processed_df = map_ipc_sections(df)\n",
        "\n",
        "    # Generate petitions for each case\n",
        "    for _, row in processed_df.iterrows():\n",
        "        case_title = row['case_title']\n",
        "        ipc_details = row['ipc_mappings']\n",
        "\n",
        "        # Draft a petition for the case\n",
        "        petition = draft_petition(\n",
        "            case_title=case_title,\n",
        "            summary=\"Detailed summary of the case goes here.\",\n",
        "            ipc_details=ipc_details,\n",
        "            prayer=\"The petitioner prays for appropriate relief, including compensation.\",\n",
        "            interim_relief=\"Stay order on eviction.\"\n",
        "        )\n",
        "        print(petition)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avne8o9upzrJ",
        "outputId": "84c7d3da-717e-4b66-a8ca-b23d68fcd292"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Case A\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    Detailed summary of the case goes here.\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    \n",
            "- Section 140:\n",
            "  - **Offense**: Wearing the dress or carrying any token used by a soldier, sailor or airman with intent that it may be believed that he is such a soldier, sailor or airman.\n",
            "  - **Punishment**: 3 Months or Fine or Both.\n",
            "  - **Description**: If someone who is not a military member wears a uniform or carries something resembling a military uniform to deceive others into believing they are a soldier, sailor, or airman, they can be punished with up to three months in jail, a fine of up to five hundred rupees, or both.\n",
            "- Section 127:\n",
            "  - **Offense**: Receiving property taken by war or depredation mentioned in sections 125 and 126.\n",
            "  - **Punishment**: 7 Years + Fine + forfeiture of property.\n",
            "  - **Description**: If someone receives property knowing it was taken during the commission of certain offenses (mentioned in sections 125 and 126), they can be punished with imprisonment of up to seven years, fined, and the property can be forfeited.\n",
            "\n",
            "**Prayer Clause:**\n",
            "- The petitioner prays for appropriate relief, including compensation.\n",
            "\n",
            "**Interim Relief Sought:**\n",
            "- Stay order on eviction.\n",
            "================================================================================\n",
            "\n",
            "    IN THE HON'BLE SUPREME COURT OF INDIA\n",
            "    CIVIL/APPELLATE/CRIMINAL JURISDICTION\n",
            "\n",
            "    PETITION UNDER ARTICLE 32/226 OF THE CONSTITUTION OF INDIA\n",
            "    \n",
            "    TITLE: Case B\n",
            "\n",
            "    **Facts of the Case:**\n",
            "    Detailed summary of the case goes here.\n",
            "\n",
            "    **Applicable Legal Provisions:**\n",
            "    \n",
            "- Section 127:\n",
            "  - **Offense**: Receiving property taken by war or depredation mentioned in sections 125 and 126.\n",
            "  - **Punishment**: 7 Years + Fine + forfeiture of property.\n",
            "  - **Description**: If someone receives property knowing it was taken during the commission of certain offenses (mentioned in sections 125 and 126), they can be punished with imprisonment of up to seven years, fined, and the property can be forfeited.\n",
            "\n",
            "**Prayer Clause:**\n",
            "- The petitioner prays for appropriate relief, including compensation.\n",
            "\n",
            "**Interim Relief Sought:**\n",
            "- Stay order on eviction.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the dataset\n",
        "data = {\n",
        "    \"Case Title\": [\n",
        "        \"S.C Jain v. Union of India\",\n",
        "        \"Aandi v. Superintendent of Police\",\n",
        "        \"Pushpa v. Maharashtra\",\n",
        "        \"Panchabhai v. Maharashtra\",\n",
        "        \"Prabodh Verma v. State of Uttar Pradesh\"\n",
        "    ],\n",
        "    \"Legal Context\": [\n",
        "        \"Payment Arrears\",\n",
        "        \"Further Investigation\",\n",
        "        \"Pension & Salary\",\n",
        "        \"Criminal Liability\",\n",
        "        \"Employee Rights\"\n",
        "    ],\n",
        "    \"Facts\": [\n",
        "        \"Arrears due to delayed payment\",\n",
        "        \"Request for detailed investigation\",\n",
        "        \"Denial of salary and pension\",\n",
        "        \"Joint actions in furtherance of intention\",\n",
        "        \"Violation of employee rights\"\n",
        "    ],\n",
        "    \"Prayer Clause\": [\n",
        "        \"Direct the respondent to release the arrears of payment with interest.\",\n",
        "        \"Order a detailed investigation under Section 173(8) of CrPC.\",\n",
        "        \"Direct the respondent to pay the salary and pension due, along with applicable interest.\",\n",
        "        \"Grant an injunction against the respondent and hold them liable under Section 34 of IPC.\",\n",
        "        \"Declare the suspension of the petitioner unlawful and direct reinstatement with full pay arrears.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "prayer_clauses_df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"/mnt/data/prayer_clauses_judgments.csv\"\n",
        "prayer_clauses_df.to_csv(output_file, index=False)\n",
        "output_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AEaMjp9lwmYz",
        "outputId": "e43d2013-667e-4b4e-9cb2-7b690cc650fd"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/prayer_clauses_judgments.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/mnt/data/prayer_clauses_judgments.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preview the data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYZ53d6Cw8WP",
        "outputId": "99415fac-ae13-4a97-a9cc-984deac0a715"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                Case Title          Legal Context  \\\n",
            "0               S.C Jain v. Union of India        Payment Arrears   \n",
            "1        Aandi v. Superintendent of Police  Further Investigation   \n",
            "2                    Pushpa v. Maharashtra       Pension & Salary   \n",
            "3                Panchabhai v. Maharashtra     Criminal Liability   \n",
            "4  Prabodh Verma v. State of Uttar Pradesh        Employee Rights   \n",
            "\n",
            "                                       Facts  \\\n",
            "0             Arrears due to delayed payment   \n",
            "1         Request for detailed investigation   \n",
            "2               Denial of salary and pension   \n",
            "3  Joint actions in furtherance of intention   \n",
            "4               Violation of employee rights   \n",
            "\n",
            "                                       Prayer Clause  \n",
            "0  Direct the respondent to release the arrears o...  \n",
            "1  Order a detailed investigation under Section 1...  \n",
            "2  Direct the respondent to pay the salary and pe...  \n",
            "3  Grant an injunction against the respondent and...  \n",
            "4  Declare the suspension of the petitioner unlaw...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define an expanded dataset with 100 cases\n",
        "cases_data = {\n",
        "    \"Case Title\": [\n",
        "        f\"Case {i} v. Respondent {i}\" for i in range(1, 101)\n",
        "    ],\n",
        "    \"Legal Context\": [\n",
        "        \"Property Dispute\" if i % 5 == 0 else\n",
        "        \"Criminal Appeal\" if i % 5 == 1 else\n",
        "        \"Labor Dispute\" if i % 5 == 2 else\n",
        "        \"Civil Rights\" if i % 5 == 3 else\n",
        "        \"Constitutional Challenge\"\n",
        "        for i in range(1, 101)\n",
        "    ],\n",
        "    \"Facts\": [\n",
        "        \"Dispute over ownership and possession of property.\" if i % 5 == 0 else\n",
        "        \"Appeal against conviction and sentence passed by lower court.\" if i % 5 == 1 else\n",
        "        \"Non-payment of wages and unfair termination.\" if i % 5 == 2 else\n",
        "        \"Violation of fundamental rights under Article 14 and 21.\" if i % 5 == 3 else\n",
        "        \"Challenge to the constitutional validity of a legislative act.\"\n",
        "        for i in range(1, 101)\n",
        "    ],\n",
        "    \"Prayer Clause\": [\n",
        "        \"Direct the respondent to transfer ownership and possession of the disputed property to the petitioner.\" if i % 5 == 0 else\n",
        "        \"Set aside the conviction and sentence and acquit the petitioner of all charges.\" if i % 5 == 1 else\n",
        "        \"Order the respondent to reinstate the petitioner and release all due wages with interest.\" if i % 5 == 2 else\n",
        "        \"Declare the actions of the respondent as unconstitutional and grant appropriate relief to the petitioner.\" if i % 5 == 3 else\n",
        "        \"Strike down the impugned legislative act as violative of the Constitution.\"\n",
        "        for i in range(1, 101)\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "expanded_prayer_clauses_df = pd.DataFrame(cases_data)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"/mnt/data/expanded_prayer_clauses_judgments.csv\"\n",
        "expanded_prayer_clauses_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Dataset with 100 cases saved to: {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu5dSS_vx86p",
        "outputId": "0d461dc6-1f81-48c5-cdb8-3e9863ccba7a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset with 100 cases saved to: /mnt/data/expanded_prayer_clauses_judgments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PrayerClauseDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data (pd.DataFrame): A DataFrame containing 'Input' and 'Prayer Clause' columns.\n",
        "            tokenizer: The tokenizer to process the text data.\n",
        "            max_length (int): The maximum sequence length for tokenization.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single sample at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing input IDs, attention masks, and labels.\n",
        "        \"\"\"\n",
        "        input_text = self.data.iloc[idx][\"Input\"]\n",
        "        label_text = self.data.iloc[idx][\"Prayer Clause\"]\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Tokenize the label text\n",
        "        labels = self.tokenizer(\n",
        "            label_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels[\"input_ids\"].squeeze(0),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "cjOjxjGUy1n6"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load Dataset (Replace with actual file path or dataset)\n",
        "data = {\n",
        "    \"Case Title\": [\n",
        "        \"S.C Jain v. Union of India\",\n",
        "        \"Aandi v. Superintendent of Police\",\n",
        "        \"Pushpa v. Maharashtra\",\n",
        "        \"Panchabhai v. Maharashtra\",\n",
        "        \"Prabodh Verma v. State of Uttar Pradesh\"\n",
        "    ],\n",
        "    \"Facts\": [\n",
        "        \"Arrears due to delayed payment\",\n",
        "        \"Request for detailed investigation\",\n",
        "        \"Denial of salary and pension\",\n",
        "        \"Joint actions in furtherance of intention\",\n",
        "        \"Violation of employee rights\"\n",
        "    ],\n",
        "    \"Prayer Clause\": [\n",
        "        \"Direct the respondent to release the arrears of payment with interest.\",\n",
        "        \"Order a detailed investigation under Section 173(8) of CrPC.\",\n",
        "        \"Direct the respondent to pay the salary and pension due, along with applicable interest.\",\n",
        "        \"Grant an injunction against the respondent and hold them liable under Section 34 of IPC.\",\n",
        "        \"Declare the suspension of the petitioner unlawful and direct reinstatement with full pay arrears.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "prayer_clauses_df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Standardize Column Names\n",
        "prayer_clauses_df.columns = prayer_clauses_df.columns.str.strip().str.title()\n",
        "\n",
        "# Step 2: Ensure Required Columns Exist\n",
        "if \"Input\" not in prayer_clauses_df.columns:\n",
        "    if \"Case Title\" in prayer_clauses_df.columns and \"Facts\" in prayer_clauses_df.columns:\n",
        "        prayer_clauses_df[\"Input\"] = prayer_clauses_df[\"Case Title\"] + \": \" + prayer_clauses_df[\"Facts\"]\n",
        "    else:\n",
        "        raise ValueError(\"Cannot create 'Input' column: Missing 'Case Title' or 'Facts' columns.\")\n",
        "\n",
        "if \"Prayer Clause\" not in prayer_clauses_df.columns:\n",
        "    raise ValueError(\"The dataset must contain a 'Prayer Clause' column.\")\n",
        "\n",
        "# Step 3: Split the Dataset\n",
        "train_data, val_data = train_test_split(prayer_clauses_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Define a Custom Dataset Class\n",
        "class PrayerClauseDataset:\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        input_text = row[\"Input\"]\n",
        "        target_text = row[\"Prayer Clause\"]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            target_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_encoding[\"input_ids\"].squeeze()\n",
        "        }\n",
        "\n",
        "# Step 5: Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Step 6: Create Dataset Instances\n",
        "train_dataset = PrayerClauseDataset(train_data, tokenizer)\n",
        "val_dataset = PrayerClauseDataset(val_data, tokenizer)\n",
        "\n",
        "# Step 7: Print Dataset Information\n",
        "print(f\"Train data size: {len(train_dataset)}\")\n",
        "print(f\"Validation data size: {len(val_dataset)}\")\n",
        "\n",
        "# Example of fetching a single data point\n",
        "example = train_dataset[0]\n",
        "print(\"Example data point:\")\n",
        "print(f\"Input IDs: {example['input_ids']}\")\n",
        "print(f\"Attention Mask: {example['attention_mask']}\")\n",
        "print(f\"Labels: {example['labels']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeaItXiBzS3c",
        "outputId": "eb755d68-59ee-4047-c09a-2228a1ae75fc"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 4\n",
            "Validation data size: 1\n",
            "Example data point:\n",
            "Input IDs: tensor([  101, 10975,  7875,  7716,  2232,  2310, 17830,  1058,  1012,  2110,\n",
            "         1997, 14940,  7970,  1024, 11371,  1997,  7904,  2916,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Labels: tensor([  101, 13520,  1996,  8636,  1997,  1996,  9964,  2121, 22300,  1998,\n",
            "         3622, 19222, 12259,  3672,  2007,  2440,  3477, 12098, 16416,  2869,\n",
            "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAkYK6EgzeMm",
        "outputId": "3b973689-48de-4c55-ba9d-4473e28e52a9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLXrUjvw0AKb",
        "outputId": "d608322b-e909-44ef-c3e2-20f74362c0bf"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Define the TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./prayer_clause_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "i_kj3yZz0Icx",
        "outputId": "fe06cb08-e22b-48c9-884b-5fab582cbacd"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.205470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.684239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.419907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=9.878597259521484, metrics={'train_runtime': 58.7935, 'train_samples_per_second': 0.204, 'train_steps_per_second': 0.051, 'total_flos': 406025404416.0, 'train_loss': 9.878597259521484, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Define Seq2SeqTrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./prayer_clause_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,  # Enable generation for predictions\n",
        "    logging_dir=\"./logs\",        # Logging directory\n",
        ")\n",
        "\n",
        "# Define Seq2SeqTrainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,  # Pass the tokenizer for sequence generation tasks\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "AS-VV8zP1HCj",
        "outputId": "0a23c4ac-e7a6-429f-fc3e-6efb86c03376"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-110-b15f100fa15b>:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.205470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.684239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.419907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=9.878597259521484, metrics={'train_runtime': 44.2925, 'train_samples_per_second': 0.271, 'train_steps_per_second': 0.068, 'total_flos': 406025404416.0, 'train_loss': 9.878597259521484, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaJm6FGP1a5r",
        "outputId": "423d5ea6-1c13-48a9-d8d8-3c52aaca8fa0"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-111-213116eaaa09>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEcIi5eS1nBi",
        "outputId": "b80946f1-20fb-4f8a-a13f-f67850f754db"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.47.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kRasSUn1qlz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}